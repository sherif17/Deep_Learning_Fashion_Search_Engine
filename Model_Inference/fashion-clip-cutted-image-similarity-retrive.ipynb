{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install super-gradients==3.1.0\n!pip install streamlit\n!pip install faiss-cpu\n!npm install localtunnel@2.0.2\n!pip install gdown\n!gdown 1-S1NsJtOiJMAp5AlxnyBXs5lOSZz_o4L","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-22T10:04:41.678566Z","iopub.execute_input":"2023-06-22T10:04:41.678929Z","iopub.status.idle":"2023-06-22T10:08:45.666639Z","shell.execute_reply.started":"2023-06-22T10:04:41.678902Z","shell.execute_reply":"2023-06-22T10:08:45.664425Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%writefile feature_extraction.py\n    \n# feature_extraction.py\nimport numpy as np\nfrom transformers import CLIPProcessor, CLIPModel\nfrom super_gradients.training import models\n\n\nclass FeatureExtractor:\n    \n    # Constructor\n    def __init__(self):\n\n        self.model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n        self.preprocess = CLIPProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n        \n    # Method to extract image features\n    def extract_image_features(self, img):\n        \n        inputs = self.preprocess.image_processor(images=img, return_tensors=\"pt\")\n        \n        outputs = self.model.get_image_features(**inputs)\n        \n        features = outputs[0].detach().numpy().reshape(1,-1)\n        \n#         features = features/np.linalg.norm(features, ord=2, axis=1, keepdims=True)\n        \n        return features\n    \n    def extract_text_features(self, txt):\n        inputs = self.preprocess.tokenizer(txt, return_tensors='pt')\n        \n        outputs = self.model.get_text_features(**inputs)\n        \n        features = outputs[0].detach().numpy().reshape(1,-1)\n        \n#         features = features/np.linalg.norm(features, ord=2, axis=1, keepdims=True)\n        \n        return features\n    \n    \nclass ClothesExtractor:\n    def __init__(self, class_dict, model_path='average_model.pth'):\n        self.model = best_model = models.get('yolo_nas_s',\n                        num_classes=44,\n                        checkpoint_path = model_path)\n        self.outputs = {}\n        self.class_dict = class_dict\n        \n\n    def extract_clothing_item(self, image):\n        \n        self.outputs['original_image'] = image\n        # get prediction\n        results = list(self.model.predict(image, conf=0.5))[0].prediction\n\n        for label_idx in set(results.labels):\n            highest_score = 0\n            for i in np.where(results.labels==label_idx)[0]:\n                XMIN, YMIN, XMAX, YMAX = results.bboxes_xyxy[i]\n                confidence = results.confidence[i]\n\n                if confidence > highest_score:\n                    highest_score = confidence\n                    image_slice = image[int(YMIN):int(YMAX), int(XMIN):int(XMAX)]\n                    self.outputs[self.class_dict[label_idx]] = image_slice\n            # Add more clothing items here as needed\n        return self.outputs","metadata":{"execution":{"iopub.status.busy":"2023-06-22T10:18:10.835769Z","iopub.execute_input":"2023-06-22T10:18:10.836205Z","iopub.status.idle":"2023-06-22T10:18:10.845024Z","shell.execute_reply.started":"2023-06-22T10:18:10.836167Z","shell.execute_reply":"2023-06-22T10:18:10.843970Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting feature_extraction.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile data_processing.py\n# data_processing.py\n\nimport time\nimport collections\nimport glob\nimport os\nimport pickle\nimport numpy as np\nimport faiss\nimport streamlit as st\n\n\ndef load_npz(file_path):\n    data = np.load(file_path, allow_pickle=True)\n    return dict(data.items())\n\ndef load_dictionary(file_path):\n    with open(file_path, 'rb') as file:\n        return pickle.load(file)\n\n@st.cache_data\ndef build_faiss_index(file_path):\n\n    if file_path.split('.')[-1] =='npz':\n        features_vector_dict = load_npz(file_path)\n    elif file_path.split('.')[-1] =='pkl':\n        features_vector_dict = load_dictionary(file_path)\n\n\n        \n#     print(list(features_vector_dict.values())[:3])\n\n    d = list(features_vector_dict.values())[0].shape[0]\n    index = faiss.IndexFlatL2(d)\n    xb = np.array(list(features_vector_dict.values())).reshape(-1, d)\n    index.add(xb)\n    \n    return index, [key.split('/')[-1] for key in features_vector_dict]\n\n# Constants\nVCTORS_PATHS = [\n    \"/kaggle/input/fashion-dataset-clip-embeddings/extracted_features_vector_all.npz\",\n    \"/kaggle/input/fashion-dataset-clip-embeddings/original_features_vector_all.npz\",\n    \"/kaggle/input/fashion-dataset-clip-embeddings/extracted_features_vector_all.pkl\",\n    \"/kaggle/input/fashion-dataset-clip-embeddings/original_features_vector_all.pkl\"\n]\n\nEXTRACTED_DIR = '/kaggle/input/fashion-dataset22/extracted_images'\nORIGINAL_DIR = '/kaggle/input/fashion-dataset22/original_images'\nMAPPING_FILE_PATH = '/kaggle/input/fashion-dataset22/name_mapping.json'\n\n\nNUM_ROWS = 2\nNUM_COLS = 5\nNUM_IMAGES = NUM_ROWS * NUM_COLS\nIMAGES_PER_ROW = NUM_COLS\nIMAGE_WIDTH = 200\n\n# Global Variables\n@st.cache_data\ndef get_paths_dict(EXTRACTED_DIR, ORIGINAL_DIR):\n    cutted_image_paths_dict = {file.split('/')[-1]: file for file in glob.glob(os.path.join(EXTRACTED_DIR, \"*/*.jpg\"))}\n    cutted_image_paths_dict = collections.OrderedDict(sorted(cutted_image_paths_dict.items()))\n\n    original_image_paths_dict = {file:os.path.join(ORIGINAL_DIR,file) for file in os.listdir(ORIGINAL_DIR)}\n    original_image_paths_dict = collections.OrderedDict(sorted(original_image_paths_dict.items()))\n\n    return original_image_paths_dict, cutted_image_paths_dict\n\noriginal_image_paths_dict, cutted_image_paths_dict = get_paths_dict(EXTRACTED_DIR, ORIGINAL_DIR)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T10:18:11.950893Z","iopub.execute_input":"2023-06-22T10:18:11.951258Z","iopub.status.idle":"2023-06-22T10:18:11.959114Z","shell.execute_reply.started":"2023-06-22T10:18:11.951231Z","shell.execute_reply":"2023-06-22T10:18:11.957759Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Overwriting data_processing.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile app.py\n# app.py\n\n\nimport streamlit as st\n# Set the app name\nst.set_page_config(page_title=\"Image Search Engine\", layout=\"wide\")\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom feature_extraction import FeatureExtractor, ClothesExtractor\nfrom data_processing import build_faiss_index, original_image_paths_dict, cutted_image_paths_dict, VCTORS_PATHS, NUM_ROWS, NUM_COLS, NUM_IMAGES, IMAGES_PER_ROW, IMAGE_WIDTH\n\n\n\ndef retrieve_similar_images(query_embedding, index, num_results=10):\n    _, closest_indices = index.search(query_embedding, num_results)\n    return closest_indices\n\ndef make_grid(rows, cols):\n    grid = [0] * rows\n    for i in range(rows):\n        with st.container():\n            grid[i] = st.columns(cols)\n    return grid\n\ndef app_layout():\n    st.title(\"Image Search Engine\")\n    st.write(\"Search for similar images by uploading an image or entering a text query.\")\n\n    search_mode = st.radio(\"Search Mode\", (\"Image\", \"Text\"))\n    \n    index_selection = st.radio(\"Select index\", (\"Original Images\", \"Cutted Images\"))\n        \n    if index_selection == \"Original Images\":\n        index, image_paths_lookup = original_index, original_image_paths_lookup\n        image_paths_dict = original_image_paths_dict\n    else:\n        # Add logic for building the index for cutted images\n        index, image_paths_lookup = cutted_index, cutted_image_paths_lookup\n        image_paths_dict = cutted_image_paths_dict\n\n    if search_mode == \"Image\":\n        uploaded_image = st.file_uploader(\"Upload an image\", type=['png', 'jpg', 'jpeg'])\n\n        if uploaded_image is not None:\n            queryImage = Image.open(uploaded_image)\n            queryImage = queryImage.convert('RGB')\n            if queryImage is not None:\n                # Extract clothing items from the uploaded image\n                \n                extracted_items = extractor.extract_clothing_item(np.array(queryImage))\n                \n                num_rows = (len(extracted_items)//NUM_COLS) + 1\n                \n\n                # Display the extracted clothing items\n#                 st.subheader(\"Uploaded Image\")\n#                 st.image(queryImage.resize((IMAGE_WIDTH, IMAGE_WIDTH)), use_column_width=True)\n\n                st.subheader(\"Extracted Clothing Items\")\n                \n                grid = make_grid(num_rows, NUM_COLS)\n                for i in range(0, len(extracted_items), NUM_COLS):\n                    st.write(\"\\n\")\n                    images_row = list(extracted_items.items())[i: i + NUM_COLS]\n                    \n                    for j, (item_name, item_image) in enumerate(images_row):\n                        grid[i // NUM_COLS][j % NUM_COLS].write(f\"**{item_name}:**\")\n                        grid[i // NUM_COLS][j % NUM_COLS].image(item_image, width=IMAGE_WIDTH)\n\n                # Let the user choose an item to use as the search image\n                selected_item = st.selectbox(\"Select an item for similarity search\", list(extracted_items.keys()) + ['All'])\n\n                if selected_item !='All':\n                    selected_image = Image.fromarray(extracted_items[selected_item])\n                    queryFeature = resnet_feature_extractor.extract_image_features(selected_image)\n\n                    if queryFeature is not None:\n                        similar_indices = retrieve_similar_images(queryFeature, index)\n                        top_10_indexes = [image_paths_lookup[idx] for idx in similar_indices[0]]\n                        top_10_similar_imgs = [image_paths_dict[img] for img in top_10_indexes]\n\n                        st.subheader(\"Similar Images\")\n\n                        grid = make_grid(NUM_ROWS, NUM_COLS)\n\n                        for i in range(0, NUM_IMAGES, IMAGES_PER_ROW):\n                            st.write(\"\\n\")\n                            images_row = top_10_similar_imgs[i: i + IMAGES_PER_ROW]\n\n                            for j, img_path in enumerate(images_row):\n                                st.write('\\t')\n                                similar_image = Image.open(img_path)\n                                similar_image = similar_image.resize((IMAGE_WIDTH, IMAGE_WIDTH))\n                                grid[i // IMAGES_PER_ROW][j % IMAGES_PER_ROW].image(similar_image, width=IMAGE_WIDTH)\n\n                        st.write(\"\\n\")\n                        \n                elif selected_item =='All':\n                    for selected_image_item in list(extracted_items.keys()):\n                        \n                        selected_image = Image.fromarray(extracted_items[selected_image_item])\n                        queryFeature = resnet_feature_extractor.extract_image_features(selected_image)   \n                        st.subheader(\"Query Image\")\n                        st.image(selected_image.resize((IMAGE_WIDTH, IMAGE_WIDTH)))\n\n                        if queryFeature is not None:\n                            similar_indices = retrieve_similar_images(queryFeature, index)\n                            top_10_indexes = [image_paths_lookup[idx] for idx in similar_indices[0]]\n                            top_10_similar_imgs = [image_paths_dict[img] for img in top_10_indexes]\n\n                            st.subheader(\"Similar Images\")\n\n                            grid = make_grid(NUM_ROWS, NUM_COLS)\n\n                            for i in range(0, NUM_IMAGES, IMAGES_PER_ROW):\n                                st.write(\"\\n\")\n                                images_row = top_10_similar_imgs[i: i + IMAGES_PER_ROW]\n\n                                for j, img_path in enumerate(images_row):\n                                    st.write('\\t')\n                                    similar_image = Image.open(img_path)\n                                    similar_image = similar_image.resize((IMAGE_WIDTH, IMAGE_WIDTH))\n                                    grid[i // IMAGES_PER_ROW][j % IMAGES_PER_ROW].image(similar_image, width=IMAGE_WIDTH)\n\n                            st.write(\"\\n\")\n\n    \n    elif search_mode == \"Text\":\n        text_query = st.text_input(\"Enter a text query\")\n        if text_query:\n        \n            queryFeature_Text = resnet_feature_extractor.extract_text_features(text_query)\n            \n            if queryFeature_Text is not None:\n                similar_indices = retrieve_similar_images(queryFeature_Text, index)\n                top_10_indexes = [image_paths_lookup[idx] for idx in similar_indices[0]]\n                top_10_similar_imgs = [image_paths_dict[img] for img in top_10_indexes]\n\n                st.subheader(\"Text Query\")\n                st.write(text_query)\n\n                st.subheader(\"Similar Images\")\n\n                grid = make_grid(NUM_ROWS, NUM_COLS)\n\n                for i in range(0, NUM_IMAGES, IMAGES_PER_ROW):\n                    st.write(\"\\n\")\n                    images_row = top_10_similar_imgs[i: i + IMAGES_PER_ROW]\n\n                    for j, img_path in enumerate(images_row):\n                        st.write('\\t')\n                        similar_image = Image.open(img_path)\n                        similar_image = similar_image.resize((IMAGE_WIDTH, IMAGE_WIDTH))\n                        grid[i // IMAGES_PER_ROW][j % IMAGES_PER_ROW].image(similar_image, width=IMAGE_WIDTH)\n\n                    st.write(\"\\n\")\n\nif __name__ == \"__main__\":\n    \n    resnet_feature_extractor = FeatureExtractor()\n    \n    clothing_dict = {0: 'Suitcase', 1: 'Miniskirt', 2: 'Tie', 3: 'Luggage & bags', 4: 'Shoe', 5: 'Belt', 6: 'Outerwear', 7: 'Dress', 8: 'Earrings', \n                 9: 'Bracelet', 10: 'Necklace', 11: 'Brassiere', 12: 'Footwear', 13: 'Satchel', 14: 'Bowtie', 15: 'Top', 16: 'Pants', 17: 'Sunglasses', \n                 18: 'Swimwear', 19: 'Clothing', 20: 'Glove', 21: 'Skirt', 22: 'High heels', 23: 'Underpants', 24: 'Fedora', 25: 'Sun hat', 26: 'Sock', \n                 27: 'Wallet', 28: 'Scarf', 29: 'Watch', 30: 'Umbrella', 31: 'Glasses', 32: 'Boot', 33: 'Basket', 34: 'Backpack', 35: 'Bag', 36: 'Hat', \n                 37: 'Coat', 38: 'Sandal', 39: 'Shorts', 40: 'Jeans', 41: 'Shirt', 42: 'Handbag', 43: 'Jacket'}\n    extractor = ClothesExtractor(class_dict=clothing_dict)\n    \n    original_index, original_image_paths_lookup = build_faiss_index(VCTORS_PATHS[3])\n    cutted_index, cutted_image_paths_lookup = build_faiss_index(VCTORS_PATHS[2])\n    app_layout()","metadata":{"execution":{"iopub.status.busy":"2023-06-22T10:57:03.034533Z","iopub.execute_input":"2023-06-22T10:57:03.035017Z","iopub.status.idle":"2023-06-22T10:57:03.050998Z","shell.execute_reply.started":"2023-06-22T10:57:03.034971Z","shell.execute_reply":"2023-06-22T10:57:03.050067Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!streamlit run app.py & npx localtunnel --port 8501","metadata":{"execution":{"iopub.status.busy":"2023-06-22T10:57:03.274663Z","iopub.execute_input":"2023-06-22T10:57:03.275261Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n\u001b[0m\nyour url is: https://five-apples-count.loca.lt\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.201.204.155:8501\u001b[0m\n\u001b[0m\nThe console stream is logged into /root/sg_logs/console.log\n[2023-06-22 10:57:28] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n[2023-06-22 10:57:35] WARNING - __init__.py - Failed to import pytorch_quantization\n[2023-06-22 10:57:35] WARNING - calibrator.py - Failed to import pytorch_quantization\n[2023-06-22 10:57:35] WARNING - export.py - Failed to import pytorch_quantization\n[2023-06-22 10:57:35] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n[2023-06-22 10:57:35] INFO - loader.py - Loading faiss with AVX2 support.\n[2023-06-22 10:57:35] INFO - loader.py - Successfully loaded faiss with AVX2 support.\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-06-22 10:59:50] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:16:18] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:16:30] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:17:08] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:29:45] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:29:56] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:30:06] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:31:37] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:32:00] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n[2023-06-22 11:32:07] INFO - checkpoint_utils.py - Successfully loaded model weights from average_model.pth EMA checkpoint.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}